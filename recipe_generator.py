# -*- coding: utf-8 -*-
"""recipe generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DGt7ErMAILEgG0kqfwVuqLGXuaZbUC2F
"""

!pip install auto-gptq
!pip install --upgrade transformers
!pip install optimum
!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from transformers import pipeline
# import streamlit as st
# import accelerate
# from transformers import AutoModelForCausalLM, AutoTokenizer\
# # Set page title and icon
# st.set_page_config(page_title="ChefBot's Recipe Generator", page_icon="üç≥")
# # Welcome message
# st.title("Welcome to FlavorFusion Recipe Generator! üë®‚Äçüç≥ü§ñ")
# st.write("Get ready to embark on a culinary adventure like no other! Whether you're a seasoned chef or a cooking novice, ChefBot is here to help you discover mouthwatering recipes that will tantalize your taste buds and ignite your passion for cooking. With ChefBot, the possibilities are endless. From comforting classics to exotic delights, we've got recipes to suit every palate and occasion. Whether you're craving comfort food on a cozy night in or preparing a gourmet feast for friends and family, ChefBot has you covered. To get started, simply tell ChefBot about your culinary preferences, dietary restrictions, and favorite ingredients. Then sit back, relax, and let ChefBot work its magic to curate personalized recipe recommendations tailored just for you. So what are you waiting for? Let's unleash your inner chef and create culinary masterpieces together! Bon app√©tit! üçΩÔ∏è‚ú®")
# 
# 
# model_name_or_path = "TheBloke/openchat-3.5-0106-GPTQ"
# 
# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,
#                                              device_map="auto",
#                                              trust_remote_code=False,
#                                              revision="main").to("cuda")
# 
# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
# system_message ='''
# Welcome to the Food Recipe Generator! üçΩÔ∏è Let's cook up something delicious together. Please tell me about your preferences, dietary restrictions, and any specific ingredients or cuisines you'd like to include in your recipe. Feel free to be as detailed as you'd like!
# 
# Here are some examples to get you started:
# 
# Dietary Preferences: Are you vegetarian, vegan, gluten-free, or following any other dietary restrictions?
# Preferred Ingredients: Do you have any favorite ingredients you'd like to include or avoid?
# Cuisine Preferences: Are you in the mood for Italian, Mexican, Asian, or any other specific cuisine?
# Cooking Time: Do you have a preference for quick and easy recipes, or are you willing to spend more time preparing a gourmet dish?
# Flavor Profile: Are you craving something savory, sweet, spicy, or tangy?
# The more information you provide, the better I can tailor the recipe suggestions to your tastes! Let's get cooking! ü•ò
# '''
# if "messages" not in st.session_state:
#     st.session_state.messages = []
# 
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])
# 
# if prompt := st.chat_input("Ask anything about cooking?"):
#     if prompt.lower() == 'stop':
#         st.write("Chatbot: Goodbye!")
#     else:
#         st.session_state.messages.append({"role": "user", "content": prompt})
#         with st.chat_message("User"):
#             st.markdown(prompt)
# 
#         with st.chat_message("Assistant"):
#             prompt_template=f'''GPT4 Correct User:{system_message}{prompt}<|end_of_turn|>GPT4 Correct Assistant:'''
#             input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()
#             output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=1024)
#             generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
# 
#             # Remove the prompt from the generated text
#             bot_response = generated_text.split('GPT4 Correct Assistant')[-1].strip()
#             st.write(bot_response)  # Display the response
#             st.session_state.messages.append({"role": "assistant", "content": bot_response})
#

!streamlit run /content/app.py &>/content/logs.txt &

!npx localtunnel --port 8501& curl ipv4.icanhazip.com